## 单词释义
function 函数  
parameter 参数  
scalar 标量  
optimization 优化  
Gradient Descent 梯度下降  
local/global minima 局部最小值/全局最小值  
curve 曲线  
error surface 误差曲面  
piecewise 分段的  
sigmoid s型的  
exponential 指数的  
batch 一批  
epoch 纪元  
HyperParameter 自己设的参数 如learning rate $\eta$、sigmoid个数、batch size  
activation function 激活函数；作用函数（个人理解：拼成整条曲线的单位函数）  
neural network 神经网络  
overfitting 过拟合  
dimension 范围；规模；维  
matrix 矩阵  
feedfullward network 卷积神经网络(个人理解：每一层layer的output作为下一层layer每个feature的input)  
binary 二进制的；二元的  
discrete 离散的  
generative model 生成模型  
prior 先验（概率）  
posterior  后面的  
sample 取样  
distribution 分布  
density 密度  
covariance 协方差  
mean 平均值  
Maximum Likelihood 极大似然估计  
transpose (矩阵)转置  
boundary 边界  
dimension 维度  
element  要素  
trivial 琐碎的  
naive 稚朴的  
determinant 行列式  
pixel 像素  
cross entropy 交叉熵  
implement 实施  
backpropagation 反相传播
data augmentation 数据扩大  

## 机器学习的基本概念
1. 用机器找到自变量为输入，应变量为理想输出的函数
2. 任务类别：
   regression（回归）： 输出为数值
   classification（分类）：输出给定选项中正确的一个
   structured learning：让机器创造一个结构（图片、文档……）
3. - 将一堆数据随机分成n个batch,称为$L_n$
   - 用每个batch更新一次$\theta$称为update
   - 将所有batch全部用过后称为1个epoch
4. Overfitting问题：
   - 训练资料上表现更好，测试资料上表现变差

## Regression
1. 步骤：
   Step 1 ：建立一个含未知参数的模型
   Step 2 : 定义loss，即界定预测值与真值的差距
   Step 3 ：优化，即找到一组未知量的值使loss最小
2. 模型：
   - linear model 线性模型：
     $$
     \underbrace{y}_{\text{model}}  = \underbrace{b}_{\text{bias}} + \underbrace{w}_{\text{weight}}\cdot\underbrace{x}_{\text{feature}} \\
    \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \Downarrow {考虑更多的feature，j为feature编号} \\
     y = b + \sum_j w_j x_j \\
     Loss:L=\frac{1}{N} \sum_n e_n \\
     e_n = \begin{cases}
            \lvert y - \hat y \rvert,MAE(absolute) \\
            (y - \hat y )^2,MSE(square)\\
            y为预测值，\hat y为真值
           \end{cases}
     $$
   - Logistic Regression（ piecewise linear model 分段线性模型 ）
     用折线拟合曲线，再将每段折线的线性模型相加
     $$
     y = b + \sum_i c_i sigmoid(b_i + w_i x_i) \\
     sigmoid(b_i + w_i x_i)= \frac{1}{1+e^{-(b_i + w_i x_i)}}\\
     \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \Downarrow {考虑更多的feature，j为feature编号} \\    
     y = b + \sum_i c_i sigmoid(b_i + \sum_j w_{ij} x_j) \\
     $$

     $$
     sigmoid \to \text{Relu (rectified linear unit，2个Relu合成一个sigmoid)} \\
     y = b + \sum_{2i} c_i max(0,b_i + \sum_j w_{ij} x_j) \\
     $$

     $$
     线代形式(矩阵运算可以用GPU加速)\\
     y = b + c^T \sigma (\bold{b}+wx)\\
      \bold{b}=\begin{pmatrix}
               b_1 \\
               b_2 \\
               \vdots\\
               b_i
               \end{pmatrix} 
      w=\begin{pmatrix}
         w_{11} & w_{12} & \cdots & w_{1j}\\
         w_{21} & w_{22} & \cdots & w_{2j}\\
         \vdots & \vdots & \ddots & \vdots\\
         w_{i1} & w_{i2} & \cdots & w_{ij}
        \end{pmatrix}
     x=\begin{pmatrix}
               x_1 \\
               x_2 \\
               \vdots\\
               x_i
               \end{pmatrix} \\   
     Loss:L(\theta)=\frac{1}{N} \sum_n e_n \\
      \theta=\begin{pmatrix}
               \theta_1 \\
               \theta_2 \\
               \vdots\\
               \theta_i
               \end{pmatrix}
      \theta 包含 \bold{b}、w、c^T、b \\
     $$
3. Loss Optimization:
   1. 先选择一个任意的 $\theta _0$
   2. 计算
      $$
      \bold{g}= \begin{bmatrix}
               \frac{\partial L}{\partial \theta _1} \vert _{\theta = \theta _0} \\
               \frac{\partial L}{\partial \theta _2} \vert _{\theta = \theta _0} \\
               \vdots
               \end{bmatrix}
             = \nabla L(\theta ^0)
      $$
   3. 更新 $\theta$
      $$
      \begin{bmatrix}
         \theta ^1 _1 \\
         \theta ^1 _2 \\
         \vdots
      \end{bmatrix}
      \gets
      \begin{bmatrix}
         \theta ^0 _1 \\
         \theta ^0 _2 \\
         \vdots
      \end{bmatrix} -
      \begin{bmatrix}
         \eta\frac{\partial L}{\partial \theta _1} \vert _{\theta = \theta _0} \\
         \eta\frac{\partial L}{\partial \theta _2} \vert _{\theta = \theta _0} \\
         \vdots 
      \end{bmatrix} = 
      \theta ^0 - \eta \bold{g} \\
      \eta \text{为设定的learning rate}
      $$ 
4. NN和DL相关概念
   - $\sigma (\bold{b}+wx)$称为Neural
   - 每个$\sigma (\bold{b}+wx)$可以作下一层$\sigma$的$x'$
   - 一组Neural称为Hidden Layer
   - 由很多组Hidden Layer组成起来的技术叫Deep Learning
5. 用Binary Regression完成Classification的任务，会惩罚特征太明显的训练数据 

## Classification
1. 步骤：
   1. 建立机率模型，其中每个class有required probability和probability distribution，probability distribution可以用Gaussian
      - 不同的class可以用相同的matrix表示$\Sigma$,减少parameter，避免overfitting，此时boundary为linear
   2. 用Maximum Likelihood寻找训练集的$\mu ^*、 \Sigma ^*$
      - Maximum Likelihood（极大似然估计）：
         $$
         L(\mu,\Sigma) = \prod_{i=1} ^n  f_{\mu,\Sigma}(x^i) \\
         f_{\mu,\Sigma}(x) = \frac{1}{(2 \pi)^{\frac{D}{2}}} \frac{1}{|\Sigma| ^\frac{1}{2}}exp \{ - \frac{1}{2}(x- \mu)^T \Sigma^{-1}(x-\mu) \} \\
         求每个class的\mu^*,\Sigma^* = arg \enspace max_{\mu,\Sigma}L(\mu,\Sigma) \\
         (\Sigma不同)\mu ^*= \frac{1}{n} \sum _n x^n, \Sigma ^*=\frac{1}{n} \sum _n (x^n - \mu ^*)(x^n - \mu ^*)^T \\
         (\Sigma相同) \Sigma ^*= \frac{N_1}{N_总} \Sigma _1 + \frac{N_2}{N_总} \Sigma _2 (N_1,N_2……为class1，class2……的元素个数；N_总 为总元素个数)
         $$

2. Naive Bayes Classification（朴素贝叶斯分布）：不model feature和feature间covariance的关系的分类方法
   - 在（feature的）不同dimension之间没有关系（independence）的情况下，表现良好
### 贝叶斯定理
1. 条件概率：
   描述的是事件 A 在另一个事件 B 已经发生条件下的概率，记作 $P(A|B)$， A 和 B 可能是相互独立的两个事件，也可能不是
   $$
   P(A|B) =  \begin{cases}
                  \frac{P(A \cap B)}{P(B)},A 和 B 可能是相互独立的两个事件，也可能不是 \\
                  \frac{P(A)P(B)}{P(B)} = P(A) ,A 和 B 是相互独立的两个事件
             \end{cases} 
   $$
2. 贝叶斯公式:
   $$
   P(A_i|B) = \frac{P(B|A_i)P(A_i)}{\sum _ j P(B|A_j)P(A_j)}
   $$
3. - $P(A)$是A的先验概率(prior)或边缘概率。之所以称为"先验"是因为它不考虑任何B方面的因素
   - $P(A|B)$是已知B发生后A的条件概率，也由于得自B的取值而被称作A的后验概率
   - $P(B|A)$是已知A发生后B的条件概率，也由于得自A的取值而被称作B的后验概率
   - $P(B)是B$的先验概率或边缘概率，也作标准化常量（normalized constant）
4. <!---->  
![使用实例](./笔记and笔记图片/贝叶斯公式示例.png)
$$
P(C_1)为Class \enspace 1的required \enspace probability \\
P(x|C_1)为Class \enspace 1产生x的probability \enspace distribution \\ 
P(C_1|x) = \frac{ P(x|C_1)P(C_1) }{ P(x|C_1)P(C_1)+P(x|C_2)P(C_2) } 
= \frac{1}{ 1+\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)} } = \frac{1}{1+exp(-z)} 
= \sigma (z) \to sigmoid \enspace function \\
z=ln \frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}
 =ln\frac{P(x|C_1)}{P(x|C_2)}+ln\frac{P(C_1)}{P(C_2)} \\
\begin{cases}
   \frac{P(C_1)}{P(C_2)} = \frac{ \frac{N_1}{N_1+N_2} }{ \frac{N_2}{N_1+N_2} } = \frac{N_1}{N_2} , \\
P(x|C_1) = \frac{1}{(2 \pi)^{\frac{D}{2}}} \frac{1}{|\Sigma ^1| ^\frac{1}{2}}exp \{ - \frac{1}{2}(x- \mu ^1)^T \Sigma^{-1}(x-\mu^1) \} ,\\
P(x|C_2) = \frac{1}{(2 \pi)^{\frac{D}{2}}} \frac{1}{|\Sigma ^2| ^\frac{1}{2}}exp \{ - \frac{1}{2}(x- \mu ^2)^T \Sigma^{-1}(x-\mu^2) \}
\end{cases}\\
z = \underbrace{( \mu ^1 - \mu ^2 ) ^T \Sigma ^{-1}}_{\bold{w}^T} x \underbrace{- \frac{1}{2}(\mu ^1)^T(\Sigma ^1)^{-1} \mu ^1 + \frac{1}{2}(\mu ^2)^T(\Sigma ^2)^{-1} \mu ^2  + ln \frac{N_1}{N_2}}_{b} (\Sigma ^1 =\Sigma ^2 = \Sigma)\\
$$

### 全概率公式
1. 若事件A1，A2，…构成一个完备事件组且都有正概率，则对任意一个事件B，有如下公式成立：
   $$
   P(B)=\sum _{i=1} ^n P(B|A_i)P(A_i) \\
   =P(BA_1)+P(BA_2)+...+P(BA_n) \\
   =P(B|A_1)P(A_1) + P(B|A_2)P(A_2) + ... + P(B|A_n)P(A_n)
   $$
2. 特别地，对于任意两随机事件A和B，有如下成立：
   $$
   P(B)=P(B|A)P(A) + P(B|\overline{A})P(\overline{A}) \\
   A和\overline{A}为对立事件
   $$

### 高斯分布（Gaussian Distribution)
$$
f_{\mu,\Sigma}(x) = \frac{1}{(2 \pi)^{\frac{D}{2}}} \frac{1}{|\Sigma| ^\frac{1}{2}}exp \{ - \frac{1}{2}(x- \mu)^T \Sigma^{-1}(x-\mu) \} (D是特征的维度)\\
mean \to \mu（D \enspace dimension \enspace vector）,covariance \to \Sigma（D*D matrix）
$$
 - $\mu$相同，$\Sigma$不同，分布机率最高点不同
 - $\mu$不同，$\Sigma$相同，离散程度不同

## model表现不佳
#### training data的loss大
1. Model Bias解决方法
   增加model弹性：
      - 更多的feature
     - deepl learning（more neurons，layers）
2. Optimization Issue(Gradient Descent找不到使loss最小的$\theta$)解决方法
   换不同的model，先train一些简单的model，或者用一些不容易出现Optimization Issue的方法
   复杂的model的model bias往往比简单的model小

#### testing data的loss大
1. overfitting: training data的loss小，testing data的loss大
   - 增加training data
   - data augmentation   
   - 给model增加限制，减少model弹性
     - 减少neuron个数
     - 公用参数
     - 减少feature